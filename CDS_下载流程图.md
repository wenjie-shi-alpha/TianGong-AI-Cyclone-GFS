# CDS 压力层数据下载流程图

```
开始下载压力层数据
    |
    ↓
检查最终文件是否存在？
    |
    ├── 是 → [跳过下载] → 返回文件路径 → 结束 ✅
    |
    └── 否 → 继续
         |
         ↓
    设置批次数 = 2
         |
         ↓
    将月份分为N个批次
    (例如：2批次 = 1-6月, 7-12月)
         |
         ↓
    逐批次下载
         |
         ├─→ 对于每个批次：
         |      |
         |      ├── 检查批次文件是否存在？
         |      |      |
         |      |      ├── 是 → 跳过此批次
         |      |      |
         |      |      └── 否 → 下载此批次
         |      |             |
         |      |             ↓
         |      |      CDS API 请求
         |      |             |
         |      |             ↓
         |      |      保存为临时文件
         |      |      (era5_pressure_..._partN.nc)
         |      |
         |      └── 所有批次完成
         |
         ↓
    所有批次都成功？
         |
         ├── 是 → 合并临时文件
         |      |
         |      ├── 使用 xarray.concat()
         |      ├── 按时间排序
         |      ├── 保存最终文件
         |      ├── 清理临时文件
         |      |
         |      └→ 返回文件路径 → 结束 ✅
         |
         └── 否 → 下载失败
                |
                ↓
           清理已下载的临时文件
                |
                ↓
           批次数 = 2？
                |
                ├── 是 → 批次数 = 3
                |      |
                |      └→ 返回到"将月份分为N个批次"
                |
                └── 否 → (批次数已经是3)
                       |
                       └→ 抛出异常 ❌
                          "即使分为3批次也无法完成下载"
```

## 关键点说明

### 1. 文件检查逻辑
- **最终文件检查**：避免重复下载整年数据
- **批次文件检查**：支持断点续传，已下载的批次不会重复下载

### 2. 批次划分示例

**2批次（一年12个月）：**
```
批次1: 1月, 2月, 3月, 4月, 5月, 6月
批次2: 7月, 8月, 9月, 10月, 11月, 12月
```

**3批次（一年12个月）：**
```
批次1: 1月, 2月, 3月, 4月
批次2: 5月, 6月, 7月, 8月
批次3: 9月, 10月, 11月, 12月
```

### 3. 自动重试机制

```python
num_splits = 2  # 初始值

while num_splits <= 3:
    try:
        # 尝试下载所有批次
        for batch in batches:
            download_batch(batch)
        
        # 成功，跳出循环
        break
        
    except Exception as e:
        # 失败，清理临时文件
        cleanup_temp_files()
        
        if num_splits == 2:
            # 增加批次数重试
            num_splits = 3
        else:
            # 已经是3批次，无法继续
            raise RuntimeError("下载失败")
```

### 4. 错误恢复

**场景1：单个批次失败**
```
批次1 ✅ → 批次2 ❌ → 批次3 (未执行)
  ↓
清理临时文件
  ↓
增加批次数到3
  ↓
批次1 ✅ → 批次2 ✅ → 批次3 ✅
  ↓
成功合并
```

**场景2：网络中断后恢复**
```
第一次运行：
  批次1 ✅ (已保存) → 批次2 ❌ (网络中断)

第二次运行：
  批次1 ✅ (检测到已存在，跳过) → 批次2 ✅ → 合并完成
```

---

## 时间估算

**单个批次下载时间：**
- 最快：5-10分钟（网络良好）
- 一般：10-20分钟
- 较慢：20-30分钟（网络不稳定）

**完整年份下载时间：**
- 2批次成功：约20-40分钟
- 3批次成功：约30-60分钟

**多年数据下载时间（5年）：**
- 地面层：5次 × 10分钟 = 50分钟
- 压力层：10-15次 × 20分钟 = 200-300分钟
- **总计：约4-6小时**

---

## 优化建议

### 对于大规模数据下载

1. **选择网络稳定时段**
   - 避开CDS服务器高峰期
   - 建议在UTC晚间（北京时间早晨）

2. **合理设置重试**
   - 如果经常失败，可以考虑直接使用3批次
   - 修改代码：`num_splits = 3  # 直接使用3批次`

3. **监控磁盘空间**
   - 每年约1-5 GB
   - 确保有足够空间存储临时文件

4. **使用断点续传**
   - 不要删除临时文件
   - 重新运行会自动检测并跳过已下载批次

---

## 代码片段参考

### 手动设置批次数
```python
# 在 download_era5_pressure_data 方法中修改
num_splits = 3  # 直接使用3批次，不尝试2批次
```

### 禁用文件检查（强制重新下载）
```python
# 临时注释掉文件检查
# if output_file.exists():
#     print(f"✅ 跳过...")
#     return str(output_file)
```

### 保留临时文件（调试用）
```python
# 在合并后不删除临时文件
# for temp_file in temp_files:
#     Path(temp_file).unlink()  # 注释掉这行
```
