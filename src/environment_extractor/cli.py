"""Command-line entry point for the environment extraction workflow."""

from __future__ import annotations

import argparse
import sys
from pathlib import Path

from .deps import ensure_available
from .pipeline import process_nc_files, streaming_from_csv


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="ä¸€ä½“åŒ–: ä¸‹è½½->è¿½è¸ª->ç¯å¢ƒåˆ†æ")
    parser.add_argument("--csv", default="output/nc_file_urls.csv", help="å«s3_urlçš„åˆ—è¡¨CSV")
    parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="é™åˆ¶å¤„ç†å‰Nä¸ªNCæ–‡ä»¶ (é»˜è®¤å¤„ç†å…¨éƒ¨)",
    )
    parser.add_argument("--nc", default=None, help="ç›´æ¥æŒ‡å®šå•ä¸ªNCæ–‡ä»¶ (è·³è¿‡ä¸‹è½½ä¸è¿½è¸ª)")
    parser.add_argument(
        "--tracks",
        default=None,
        help="ç›´æ¥æŒ‡å®šè½¨è¿¹CSV (è·³è¿‡è¿½è¸ª)\nè‹¥ä¸--ncåŒæ—¶ç»™å‡ºåˆ™åªåšç¯å¢ƒåˆ†æ",
    )
    parser.add_argument("--no-clean", action="store_true", help="åˆ†æåä¸åˆ é™¤NC")
    parser.add_argument("--keep-nc", action="store_true", help="åŒ --no-clean (å…¼å®¹)")
    parser.add_argument("--auto", action="store_true", help="æ— è½¨è¿¹åˆ™è‡ªåŠ¨è¿è¡Œè¿½è¸ª")
    parser.add_argument("--search-range", type=float, default=3.0, help="è¿½è¸ªæœç´¢èŒƒå›´")
    parser.add_argument("--memory", type=int, default=3, help="è¿½è¸ªè®°å¿†æ—¶é—´æ­¥")
    parser.add_argument(
        "--initials",
        default=str(Path("input") / "western_pacific_typhoons_superfast.csv"),
        help="initialTracker åˆå§‹ç‚¹CSV",
    )
    parser.add_argument(
        "--batch",
        action="store_true",
        help="ä½¿ç”¨æ—§çš„æ‰¹é‡æ¨¡å¼: å…ˆå…¨éƒ¨ä¸‹è½½+è¿½è¸ª, å†ç»Ÿä¸€åšç¯å¢ƒåˆ†æ",
    )
    parser.add_argument(
        "--processes",
        type=int,
        default=1,
        help="å¹¶è¡Œè¿è¡Œçš„è¿›ç¨‹æ•° (>=1)ã€‚æœ€å¤§å¹¶è¡Œä»»åŠ¡æ•°ä¸è¿›ç¨‹æ•°ä¸€è‡´",
    )
    parser.add_argument(
        "--concise-log",
        action="store_true",
        help="å¯ç”¨ç²¾ç®€æ—¥å¿—æ¨¡å¼ï¼Œä»…è¾“å‡ºæ–‡ä»¶å®Œæˆæƒ…å†µ",
    )
    return parser


def _prepare_batch_targets(
    csv_path: Path, limit: int | None, initials_csv: Path, concise_log: bool = False
) -> list[Path]:
    import pandas as pd

    from initialTracker import track_file_with_initials as it_track_file_with_initials
    from initialTracker import _load_all_points as it_load_all_points

    from .workflow_utils import (
        combine_initial_tracker_outputs,
        download_s3_public,
        extract_forecast_tag,
        sanitize_filename,
    )

    def detail(message: str) -> None:
        if not concise_log:
            print(message)

    def summary(message: str) -> None:
        print(message)

    if not csv_path.exists():
        summary(f"âŒ CSVä¸å­˜åœ¨: {csv_path}")
        sys.exit(1)

    df = pd.read_csv(csv_path)
    required_cols = {"s3_url", "model_prefix", "init_time"}
    if not required_cols.issubset(df.columns):
        missing = required_cols - set(df.columns)
        summary(f"âŒ CSVç¼ºå°‘å¿…è¦åˆ—: {missing}")
        sys.exit(1)

    if limit is not None:
        df = df.head(limit)

    persist_dir = Path("data/nc_files")
    persist_dir.mkdir(parents=True, exist_ok=True)
    track_dir = Path("track_single")
    track_dir.mkdir(exist_ok=True)

    if initials_csv.exists():
        initials_path = initials_csv
    else:
        fallback = Path("input/western_pacific_typhoons_superfast.csv")
        if fallback.exists():
            summary(f"âš ï¸ æŒ‡å®šåˆå§‹ç‚¹æ–‡ä»¶ä¸å­˜åœ¨, ä½¿ç”¨é»˜è®¤: {fallback}")
            initials_path = fallback
        else:
            summary(f"âŒ æ‰¾ä¸åˆ°åˆå§‹ç‚¹CSV: {initials_csv}")
            sys.exit(1)
    initials_df = it_load_all_points(initials_path)

    prepared: list[Path] = []

    def remove_nc_file(path: Path, reason: str) -> None:
        """åˆ é™¤æ— æ³•ç”¨äºåç»­åˆ†æçš„ NC æ–‡ä»¶ã€‚"""
        try:
            path.unlink()
            detail(f"ğŸ§¹ å·²åˆ é™¤NC ({reason})")
        except FileNotFoundError:
            pass
        except Exception as exc:
            summary(f"âš ï¸ åˆ é™¤NCå¤±è´¥({reason}): {exc}")
    detail(f"â¬‡ï¸ [æ‰¹é‡æ¨¡å¼] é€é¡¹ä¸‹è½½ä¸è¿½è¸ª (limit={limit})")
    for idx, row in df.iterrows():
        s3_url = row["s3_url"]
        model_prefix = row["model_prefix"]
        init_time = row["init_time"]
        fname = Path(s3_url).name
        forecast_tag = extract_forecast_tag(fname)
        safe_prefix = sanitize_filename(model_prefix)
        safe_init = sanitize_filename(init_time.replace(":", "").replace("-", ""))
        combined_track_csv = track_dir / f"tracks_{safe_prefix}_{safe_init}_{forecast_tag}.csv"
        nc_local = persist_dir / fname
        nc_stem = nc_local.stem

        detail(f"\n[{idx+1}/{len(df)}] â–¶ï¸ å¤„ç†: {fname}")

        if not nc_local.exists():
            try:
                detail(f"â¬‡ï¸  ä¸‹è½½NC: {s3_url}")
                download_s3_public(s3_url, nc_local)
            except Exception as exc:
                summary(f"âŒ ä¸‹è½½å¤±è´¥: {exc}")
                continue
        else:
            detail("ğŸ“¦ å·²å­˜åœ¨NCæ–‡ä»¶, å¤ç”¨")

        track_csv: Path | None = None

        if combined_track_csv.exists():
            track_csv = combined_track_csv
            detail("ğŸ—ºï¸  å·²å­˜åœ¨è½¨è¿¹CSV, è·³è¿‡è¿½è¸ª")
        else:
            single_candidates = sorted(track_dir.glob(f"track_*_{nc_stem}.csv"))
            if len(single_candidates) == 1:
                try:
                    combined = combine_initial_tracker_outputs(single_candidates, nc_local)
                    if combined is not None and not combined.empty:
                        combined.to_csv(single_candidates[0], index=False)
                    track_csv = single_candidates[0]
                    detail("ğŸ—ºï¸  å‘ç°å•æ¡è½¨è¿¹æ–‡ä»¶, å·²æ›´æ–°åç›´æ¥ä½¿ç”¨")
                except Exception as exc:
                    summary(f"âš ï¸ å•è½¨è¿¹æ–‡ä»¶æ ¼å¼æ›´æ–°å¤±è´¥: {exc}")
            elif len(single_candidates) > 1:
                try:
                    combined = combine_initial_tracker_outputs(single_candidates, nc_local)
                    if combined is not None and not combined.empty:
                        combined.to_csv(combined_track_csv, index=False)
                        track_csv = combined_track_csv
                        detail(
                            f"ğŸ—ºï¸  å‘ç°å¤šæ¡å•ç‹¬è½¨è¿¹æ–‡ä»¶, å·²åˆå¹¶ç”Ÿæˆ {combined_track_csv.name}"
                        )
                except Exception as exc:
                    summary(f"âš ï¸ åˆå¹¶å·²æœ‰è½¨è¿¹å¤±è´¥: {exc}")

        if track_csv is not None:
            prepared.append(nc_local)
            continue

        try:
            per_storm = it_track_file_with_initials(nc_local, initials_df, track_dir)
            if not per_storm:
                detail("âš ï¸ æ— æœ‰æ•ˆè½¨è¿¹ -> åˆ é™¤NC")
                remove_nc_file(nc_local, "æ— è½¨è¿¹")
                continue
            combined = combine_initial_tracker_outputs(per_storm, nc_local)
            if combined is None or combined.empty:
                detail("âš ï¸ åˆå¹¶è½¨è¿¹å¤±è´¥ -> åˆ é™¤NC")
                remove_nc_file(nc_local, "æ— è½¨è¿¹")
                continue

            if combined["particle"].nunique() == 1:
                single_path = Path(per_storm[0])
                combined.to_csv(single_path, index=False)
                track_csv = single_path
                detail(f"ğŸ’¾ ä¿å­˜å•æ¡è½¨è¿¹: {single_path.name}")
                if combined_track_csv.exists():
                    try:
                        combined_track_csv.unlink()
                    except Exception:
                        pass
            else:
                combined.to_csv(combined_track_csv, index=False)
                track_csv = combined_track_csv
                detail(
                    f"ğŸ’¾ åˆå¹¶ä¿å­˜è½¨è¿¹: {combined_track_csv.name} (å« {combined['particle'].nunique()} æ¡è·¯å¾„)"
                )
        except Exception as exc:
            summary(f"âŒ è¿½è¸ªå¤±è´¥: {exc}")
            remove_nc_file(nc_local, "è¿½è¸ªå¤±è´¥")
            continue

        if track_csv is None:
            remove_nc_file(nc_local, "æ— è½¨è¿¹")
            continue

        prepared.append(nc_local)

    if not prepared:
        summary("âŒ æœªæˆåŠŸå‡†å¤‡ä»»ä½•NCæ–‡ä»¶")
        sys.exit(1)

    return prepared


def main(argv: list[str] | None = None) -> None:
    parser = build_parser()
    args = parser.parse_args(argv)

    ensure_available()

    def detail(message: str) -> None:
        if not args.concise_log:
            print(message)

    def summary(message: str) -> None:
        print(message)

    logs_root = Path("final_single_output") / "logs"

    detail("ğŸŒ€ ä¸€ä½“åŒ–çƒ­å¸¦æ°”æ—‹åˆ†ææµç¨‹å¯åŠ¨")
    detail("=" * 60)

    if args.nc:
        nc_path = Path(args.nc)
        if not nc_path.exists():
            summary(f"âŒ æŒ‡å®šNCä¸å­˜åœ¨: {nc_path}")
            sys.exit(1)
        target_nc_files = [nc_path]
        detail("ğŸ“¦ å•æ–‡ä»¶åˆ†ææ¨¡å¼")
    else:
        if args.batch:
            target_nc_files = _prepare_batch_targets(
                Path(args.csv), args.limit, Path(args.initials), args.concise_log
            )
            detail(f"ğŸ“¦ å¾…ç¯å¢ƒåˆ†æNCæ•°é‡: {len(target_nc_files)}")
        else:
            detail("ğŸšš å¯ç”¨æµå¼é¡ºåºå¤„ç†: æ¯ä¸ªNCç‹¬ç«‹å®Œæˆ(ä¸‹è½½->è¿½è¸ª->ç¯å¢ƒåˆ†æ->æ¸…ç†)")
            streaming_from_csv(
                csv_path=Path(args.csv),
                limit=args.limit,
                search_range=args.search_range,
                memory=args.memory,
                keep_nc=(args.no_clean or args.keep_nc),
                initials_csv=Path(args.initials) if args.initials else None,
                processes=max(1, args.processes),
                concise_log=args.concise_log,
                logs_root=logs_root,
            )
            detail("ğŸ¯ æµå¼å¤„ç†å®Œæˆ (æ— éœ€è¿›å…¥æ‰¹é‡åå¤„ç†å¾ªç¯)")
            return

    process_nc_files(
        target_nc_files,
        args,
        concise_log=args.concise_log,
        logs_root=logs_root,
    )


__all__ = ["main", "build_parser"]
