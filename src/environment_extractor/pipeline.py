"""Pipeline orchestration for the tropical cyclone environment extractor."""

from __future__ import annotations

import json
from collections import defaultdict
from contextlib import redirect_stderr, redirect_stdout
from datetime import datetime
from pathlib import Path
from typing import Iterable

_MANIFEST_FILENAME = "_analysis_manifest.json"


def _manifest_path(output_dir: Path) -> Path:
    return output_dir / _MANIFEST_FILENAME


def _load_manifest_index(output_dir: Path) -> dict[str, set[str]] | None:
    manifest = _manifest_path(output_dir)
    if not manifest.exists():
        return None
    try:
        dir_mtime = output_dir.stat().st_mtime
        manifest_mtime = manifest.stat().st_mtime
        if dir_mtime - manifest_mtime > 1e-6:
            return None
        with manifest.open("r", encoding="utf-8") as fh:
            payload = json.load(fh)
    except (OSError, json.JSONDecodeError, TypeError, ValueError):
        return None

    entries = payload.get("entries")
    if not isinstance(entries, dict):
        return None

    index: dict[str, set[str]] = {}
    for stem, particles in entries.items():
        if not isinstance(particles, list):
            continue
        normalized = {str(p) for p in particles if p}
        if normalized:
            index[str(stem)] = normalized
    return index


def _persist_manifest_index(output_dir: Path, index: dict[str, set[str]]) -> None:
    manifest = _manifest_path(output_dir)
    payload = {
        "generated_at": datetime.utcnow().isoformat(),
        "entries": {stem: sorted(particles) for stem, particles in sorted(index.items())},
    }
    try:
        manifest.parent.mkdir(parents=True, exist_ok=True)
        with manifest.open("w", encoding="utf-8") as fh:
            json.dump(payload, fh, ensure_ascii=False, indent=2)
    except OSError:
        pass


def _register_manifest_entries(
    index: dict[str, set[str]],
    output_dir: Path,
    nc_stem: str,
    particles: Iterable[str],
) -> None:
    incoming = {str(p) for p in particles if p}
    if not incoming:
        return

    current = index.get(nc_stem)
    if current is None:
        index[nc_stem] = incoming
        _persist_manifest_index(output_dir, index)
        return

    before = len(current)
    current.update(incoming)
    if len(current) != before:
        _persist_manifest_index(output_dir, index)

from .extractor import TCEnvironmentalSystemsExtractor
from .workflow_utils import (
    combine_initial_tracker_outputs,
    download_s3_public,
    extract_forecast_tag,
    sanitize_filename,
)


def _index_existing_json(output_dir: Path) -> dict[str, set[str]]:
    """Load cached index of existing JSON outputs, falling back to a directory scan."""

    manifest_index = _load_manifest_index(output_dir)
    if manifest_index is not None:
        return manifest_index

    index: dict[str, set[str]] = defaultdict(set)
    pattern = "*_TC_Analysis_*.json"
    for json_path in output_dir.glob(pattern):
        try:
            if json_path.stat().st_size <= 10:
                continue
        except OSError:
            continue
        stem = json_path.stem
        if "_TC_Analysis_" not in stem:
            continue
        nc_stem, particle = stem.split("_TC_Analysis_", 1)
        if particle:
            index[nc_stem].add(particle)

    dense_index = {k: set(v) for k, v in index.items() if v}
    _persist_manifest_index(output_dir, dense_index)
    return dense_index
def _run_environment_analysis(
    nc_path: str,
    track_csv: str,
    output_dir: str,
    keep_nc: bool,
    log_file: str | None = None,
    concise: bool = False,
) -> tuple[bool, str | None, set[str]]:
    """Worker helper executed in a child process for ç¯å¢ƒåˆ†æ."""

    success = False
    error_message: str | None = None
    completed_particles: set[str] = set()
    nc_name = Path(nc_path).name

    def _execute() -> None:
        nonlocal success, error_message, completed_particles

        if not concise:
            print(f"[{datetime.utcnow().isoformat()}] â–¶ï¸ ç¯å¢ƒåˆ†æå¼€å§‹: {nc_name}")
        try:
            with TCEnvironmentalSystemsExtractor(nc_path, track_csv) as extractor:
                result = extractor.analyze_and_export_as_json(output_dir)
                success = True
                if isinstance(result, dict):
                    completed_particles = {str(key) for key in result.keys()}
                print(f"[{datetime.utcnow().isoformat()}] âœ… ç¯å¢ƒåˆ†æå®Œæˆ: {nc_name}")
        except Exception as exc:  # pragma: no cover - worker side error path
            error_message = str(exc)
            print(f"[{datetime.utcnow().isoformat()}] âŒ ç¯å¢ƒåˆ†æå¤±è´¥: {nc_name} -> {error_message}")
        finally:
            if not keep_nc:
                try:
                    Path(nc_path).unlink()
                    print(f"[{datetime.utcnow().isoformat()}] ğŸ§¹ å·²åˆ é™¤NCæ–‡ä»¶: {nc_name}")
                except FileNotFoundError:
                    pass
                except Exception as exc:
                    if success:
                        success = False
                        error_message = f"åˆ é™¤NCå¤±è´¥: {exc}"
                    print(
                        f"[{datetime.utcnow().isoformat()}] âš ï¸ åˆ é™¤NCå¤±è´¥ ({nc_name}): {exc}"
                    )

    if log_file and not concise:
        log_path = Path(log_file)
        log_path.parent.mkdir(parents=True, exist_ok=True)
        with log_path.open("w", encoding="utf-8") as fh:
            fh.write(f"æ—¥å¿—æ–‡ä»¶: {nc_name}\n")
            fh.flush()
            with redirect_stdout(fh), redirect_stderr(fh):
                _execute()
    else:
        _execute()

    return success, error_message, completed_particles


# ================= æ–°å¢: æµå¼é¡ºåºå¤„ç†å‡½æ•° =================
def streaming_from_csv(
    csv_path: Path,
    limit: int | None = None,
    search_range: float = 3.0,
    memory: int = 3,
    keep_nc: bool = False,
    initials_csv: Path | None = None,
    processes: int = 1,
    max_in_flight: int = 2,
    concise_log: bool = False,
    logs_root: Path | None = None,
): 
    """é€è¡Œè¯»å–CSV, æ¯ä¸ªNCæ–‡ä»¶æ‰§è¡Œ: ä¸‹è½½ -> è¿½è¸ª -> ç¯å¢ƒåˆ†æ -> (å¯é€‰åˆ é™¤)

    ä¸åŸæ‰¹é‡æ¨¡å¼æœ€å¤§åŒºåˆ«: ä¸é¢„å…ˆä¸‹è½½å…¨éƒ¨; æ¯ä¸ªæ–‡ä»¶å®Œæˆåå³å¯é‡Šæ”¾ç£ç›˜ã€‚
    """
    def detail(message: str) -> None:
        if not concise_log:
            print(message)

    def summary(message: str) -> None:
        print(message)

    if not csv_path.exists():
        summary(f"âŒ CSVä¸å­˜åœ¨: {csv_path}")
        return
    import pandas as pd, traceback
    from concurrent.futures import FIRST_COMPLETED, Future, ProcessPoolExecutor, wait
    from initialTracker import track_file_with_initials as it_track_file_with_initials
    from initialTracker import _load_all_points as it_load_initial_points

    df = pd.read_csv(csv_path)
    required_cols = {"s3_url", "model_prefix", "init_time"}
    if not required_cols.issubset(df.columns):
        summary(f"âŒ CSVç¼ºå°‘å¿…è¦åˆ—: {required_cols - set(df.columns)}")
        return
    if limit is not None:
        df = df.head(limit)

    processes = max(1, int(processes))
    max_in_flight = processes

    persist_dir = Path("data/nc_files")
    persist_dir.mkdir(parents=True, exist_ok=True)
    track_dir = Path("track_single")
    track_dir.mkdir(exist_ok=True)
    final_dir = Path("final_single_output")
    final_dir.mkdir(exist_ok=True)

    existing_index = _index_existing_json(final_dir)
    original_total = len(df)
    pre_skipped = 0
    if original_total:
        df = df.assign(_nc_stem=df["s3_url"].map(lambda url: Path(url).stem))
        if existing_index:
            pre_mask = df["_nc_stem"].map(lambda stem: bool(existing_index.get(stem)))
            pre_skipped = int(pre_mask.sum())
            if pre_skipped:
                summary(f"â­ï¸ é¢„æ£€è·³è¿‡ {pre_skipped} ä¸ªå·²æœ‰ JSON çš„æ¡ç›®")
            df = df.loc[~pre_mask].copy()
        else:
            df = df.copy()
    else:
        df = df.copy()

    if df.empty:
        summary("â¹ï¸ æ‰€æœ‰æ¡ç›®å·²æœ‰ JSON ç»“æœï¼Œæµç¨‹æå‰ç»“æŸã€‚")
        summary(f"ğŸ“ è¾“å‡ºç›®å½•: {final_dir}")
        return

    detail(
        f"ğŸ“„ æµå¼å¾…å¤„ç†æ•°é‡: {len(df)} (limit={limit}, åŸå§‹={original_total})"
    )

    parallel = processes > 1
    executor: ProcessPoolExecutor | None = None
    active_futures: dict[Future, dict[str, str]] = {}
    logs_dir: Path | None = None
    if logs_root is not None and parallel and not concise_log:
        logs_dir = logs_root
        logs_dir.mkdir(parents=True, exist_ok=True)

    processed = 0
    skipped = pre_skipped

    if parallel:
        detail(
            f"âš™ï¸ å·²å¯ç”¨å¹¶è¡Œç¯å¢ƒåˆ†æ: è¿›ç¨‹æ•°={processes}, æ¯æ¬¡æœ€å¤šå¹¶è¡Œ{max_in_flight}ä¸ªæ–‡ä»¶"
        )
        executor = ProcessPoolExecutor(max_workers=processes)

    def drain_completed(block: bool) -> None:
        nonlocal processed
        if not parallel or not active_futures:
            return

        futures = list(active_futures.keys())
        if block:
            done, _ = wait(futures, return_when=FIRST_COMPLETED)
        else:
            done = {f for f in futures if f.done()}
            if not done:
                return

        for fut in done:
            meta = active_futures.pop(fut, {})
            label = meta.get("label", "æœªçŸ¥æ–‡ä»¶")
            try:
                success, error_msg, produced = fut.result()
            except Exception as exc:  # pragma: no cover - defensive guard
                success = False
                error_msg = str(exc)
                produced = set()
            if success:
                processed += 1
                summary(f"âœ… ç¯å¢ƒåˆ†æå®Œæˆ: {label}")
                stem = meta.get("stem")
                if stem:
                    _register_manifest_entries(existing_index, final_dir, stem, produced)
            else:
                log_hint = meta.get("log")
                extra = f" -> {error_msg}" if error_msg else ""
                if log_hint:
                    summary(
                        f"âŒ ç¯å¢ƒåˆ†æå¤±è´¥: {label}{extra} (è¯¦è§ {log_hint})"
                    )
                else:
                    summary(f"âŒ ç¯å¢ƒåˆ†æå¤±è´¥: {label}{extra}")

    def ensure_capacity() -> None:
        if not parallel:
            return
        while len(active_futures) >= max_in_flight:
            drain_completed(block=True)

    try:
        for idx, row in df.iterrows():
            if parallel:
                drain_completed(block=False)
                ensure_capacity()

            s3_url = row["s3_url"]
            model_prefix = row["model_prefix"]
            init_time = row["init_time"]
            fname = Path(s3_url).name
            forecast_tag = extract_forecast_tag(fname)
            safe_prefix = sanitize_filename(model_prefix)
            safe_init = sanitize_filename(init_time.replace(":", "").replace("-", ""))
            combined_track_csv = track_dir / f"tracks_{safe_prefix}_{safe_init}_{forecast_tag}.csv"
            nc_local = persist_dir / fname
            nc_stem = row.get("_nc_stem", nc_local.stem)

            detail(f"\n[{idx+1}/{len(df)}] â–¶ï¸ å¤„ç†: {fname}")

            existing_particles = existing_index.get(nc_stem, set())
            if existing_particles:
                detail(f"â­ï¸  å·²å­˜åœ¨æœ€ç»ˆJSON({len(existing_particles)}) -> è·³è¿‡")
                skipped += 1
                continue

            if not nc_local.exists():
                try:
                    detail(f"â¬‡ï¸  ä¸‹è½½NC: {s3_url}")
                    download_s3_public(s3_url, nc_local)
                except Exception as e:
                    summary(f"âŒ ä¸‹è½½å¤±è´¥, è·³è¿‡: {e}")
                    skipped += 1
                    continue
            else:
                detail("ğŸ“¦ å·²å­˜åœ¨NCæ–‡ä»¶, å¤ç”¨")

            track_csv: Path | None = None

            if combined_track_csv.exists():
                track_csv = combined_track_csv
                detail("ğŸ—ºï¸  å·²å­˜åœ¨è½¨è¿¹CSV, ç›´æ¥ç¯å¢ƒåˆ†æ")
            else:
                single_candidates = sorted(track_dir.glob(f"track_*_{nc_stem}.csv"))
                if len(single_candidates) == 1:
                    try:
                        combined = combine_initial_tracker_outputs(single_candidates, nc_local)
                        if combined is not None and not combined.empty:
                            combined.to_csv(single_candidates[0], index=False)
                        track_csv = single_candidates[0]
                        detail("ğŸ—ºï¸  å‘ç°å•æ¡è½¨è¿¹æ–‡ä»¶, å·²æ›´æ–°åç›´æ¥ä½¿ç”¨")
                    except Exception as e:
                        summary(f"âš ï¸ å•è½¨è¿¹æ–‡ä»¶æ ¼å¼æ›´æ–°å¤±è´¥: {e}")
                elif len(single_candidates) > 1:
                    try:
                        combined = combine_initial_tracker_outputs(single_candidates, nc_local)
                        if combined is not None and not combined.empty:
                            combined.to_csv(combined_track_csv, index=False)
                            track_csv = combined_track_csv
                            detail(
                                f"ğŸ—ºï¸  å‘ç°å¤šæ¡å•ç‹¬è½¨è¿¹æ–‡ä»¶, å·²åˆå¹¶ç”Ÿæˆ {combined_track_csv.name}"
                            )
                    except Exception as e:
                        summary(f"âš ï¸ åˆå¹¶å·²æœ‰è½¨è¿¹å¤±è´¥: {e}")

            if track_csv is None:
                try:
                    detail("ğŸ§­ ä½¿ç”¨ initialTracker æ‰§è¡Œè¿½è¸ª...")
                    initials_path = initials_csv or Path("input/western_pacific_typhoons_superfast.csv")
                    initials_df = it_load_initial_points(initials_path)
                    per_storm_csvs = it_track_file_with_initials(
                        Path(nc_local), initials_df, track_dir
                    )
                    if not per_storm_csvs:
                        detail("âš ï¸ æ— æœ‰æ•ˆè½¨è¿¹ -> è·³è¿‡ç¯å¢ƒåˆ†æ")
                        if not keep_nc:
                            try:
                                nc_local.unlink()
                                detail("ğŸ§¹ å·²åˆ é™¤NC (æ— è½¨è¿¹)")
                            except Exception:
                                pass
                        skipped += 1
                        continue

                    combined = combine_initial_tracker_outputs(per_storm_csvs, nc_local)
                    if combined is None or combined.empty:
                        detail("âš ï¸ æ— æ³•åˆå¹¶è½¨è¿¹è¾“å‡º -> è·³è¿‡ç¯å¢ƒåˆ†æ")
                        if not keep_nc:
                            try:
                                nc_local.unlink()
                                detail("ğŸ§¹ å·²åˆ é™¤NC (æ— è½¨è¿¹)")
                            except Exception:
                                pass
                        skipped += 1
                        continue

                    if combined["particle"].nunique() == 1:
                        single_path = Path(per_storm_csvs[0])
                        combined.to_csv(single_path, index=False)
                        track_csv = single_path
                        detail(f"ğŸ’¾ ä¿å­˜å•æ¡è½¨è¿¹: {single_path.name}")
                        if combined_track_csv.exists():
                            try:
                                combined_track_csv.unlink()
                            except Exception:
                                pass
                    else:
                        combined.to_csv(combined_track_csv, index=False)
                        track_csv = combined_track_csv
                        detail(
                            f"ğŸ’¾ åˆå¹¶ä¿å­˜è½¨è¿¹: {combined_track_csv.name} (å« {combined['particle'].nunique()} æ¡è·¯å¾„)"
                        )
                except Exception as e:
                    summary(f"âŒ è¿½è¸ªå¤±è´¥: {e}")
                    if not concise_log:
                        traceback.print_exc()
                    if not keep_nc:
                        try:
                            nc_local.unlink()
                            detail("ğŸ§¹ å·²åˆ é™¤NC (è¿½è¸ªå¤±è´¥)")
                        except Exception:
                            pass
                    skipped += 1
                    continue

            if track_csv is None:
                detail("âš ï¸ æœªèƒ½ç”Ÿæˆæœ‰æ•ˆè½¨è¿¹ -> è·³è¿‡ç¯å¢ƒåˆ†æ")
                if not keep_nc:
                    try:
                        nc_local.unlink()
                        detail("ğŸ§¹ å·²åˆ é™¤NC (æ— è½¨è¿¹)")
                    except FileNotFoundError:
                        pass
                    except Exception as exc:
                        summary(f"âš ï¸ åˆ é™¤NCå¤±è´¥: {exc}")
                skipped += 1
                continue

            if parallel and executor:
                detail("ğŸ§® å·²æäº¤ç¯å¢ƒåˆ†æä»»åŠ¡ (å¹¶è¡Œ)")
                log_file = (
                    str((logs_dir / f"{nc_local.stem}.log").resolve())
                    if logs_dir is not None and not concise_log
                    else None
                )
                future = executor.submit(
                    _run_environment_analysis,
                    str(nc_local),
                    str(track_csv),
                    "final_single_output",
                    keep_nc,
                    log_file,
                    concise_log,
                )
                meta: dict[str, str] = {"label": nc_local.name, "stem": nc_stem}
                if log_file:
                    meta["log"] = log_file
                active_futures[future] = meta
            else:
                try:
                    success, error_msg, produced = _run_environment_analysis(
                        str(nc_local),
                        str(track_csv),
                        "final_single_output",
                        keep_nc,
                        None,
                        concise_log,
                    )
                    if success:
                        processed += 1
                        _register_manifest_entries(existing_index, final_dir, nc_stem, produced)
                    elif error_msg:
                        summary(f"âŒ ç¯å¢ƒåˆ†æå¤±è´¥: {error_msg}")
                except Exception as e:
                    summary(f"âŒ ç¯å¢ƒåˆ†æå¤±è´¥: {e}")

    finally:
        if parallel and executor:
            while active_futures:
                drain_completed(block=True)
            executor.shutdown(wait=True)

    summary("\nğŸ“Š æµå¼å¤„ç†ç»“æœ:")
    summary(f"  âœ… å®Œæˆ: {processed}")
    summary(f"  â­ï¸ è·³è¿‡: {skipped}")
    summary(f"  ğŸ“ è¾“å‡ºç›®å½•: final_single_output")


def process_nc_files(
    target_nc_files,
    args,
    concise_log: bool = False,
    logs_root: Path | None = None,
):
    """å¤„ç†å·²å‡†å¤‡å¥½çš„ NC æ–‡ä»¶åˆ—è¡¨ï¼Œä¿æŒ legacy è¡Œä¸ºä¸å˜ã€‚"""
    import pandas as pd
    from concurrent.futures import FIRST_COMPLETED, Future, ProcessPoolExecutor, wait

    def detail(message: str) -> None:
        if not concise_log:
            print(message)

    def summary(message: str) -> None:
        print(message)

    target_nc_files = list(target_nc_files)
    final_output_dir = Path("final_single_output")
    final_output_dir.mkdir(exist_ok=True)

    existing_index = _index_existing_json(final_output_dir)
    original_total = len(target_nc_files)
    pre_skipped = 0
    if target_nc_files:
        filtered: list[Path] = []
        for nc_path in target_nc_files:
            stem = nc_path.stem
            if existing_index.get(stem):
                pre_skipped += 1
            else:
                filtered.append(nc_path)
        if pre_skipped:
            summary(f"â­ï¸ é¢„æ£€è·³è¿‡ {pre_skipped} ä¸ªå·²æœ‰ JSON çš„ NC æ–‡ä»¶")
        target_nc_files = filtered

    if not target_nc_files:
        summary("â¹ï¸ æ‰€æœ‰ NC å·²å­˜åœ¨åˆ†æç»“æœï¼Œè·³è¿‡æ‰¹é‡å¤„ç†ã€‚")
        summary(f"ğŸ“ è¾“å‡ºç›®å½•: {final_output_dir}")
        return 0, pre_skipped

    processes = max(1, int(getattr(args, "processes", 1)))
    max_in_flight = processes
    parallel = processes > 1
    executor: ProcessPoolExecutor | None = None
    active_futures: dict[Future, dict[str, str]] = {}

    if parallel:
        detail(
            f"âš™ï¸ å¹¶è¡Œç¯å¢ƒåˆ†æå·²å¯ç”¨ (è¿›ç¨‹æ•°={processes}, æ¯æ¬¡æœ€å¤š{max_in_flight}ä¸ªæ–‡ä»¶)"
        )
        executor = ProcessPoolExecutor(max_workers=processes)

    keep_nc_flag = bool(getattr(args, "no_clean", False) or getattr(args, "keep_nc", False))

    def remove_nc_file(path: Path, reason: str) -> None:
        if keep_nc_flag:
            return
        try:
            path.unlink()
            detail(f"ğŸ§¹ å·²åˆ é™¤ NC ({reason}): {path.name}")
        except FileNotFoundError:
            pass
        except Exception as exc:
            summary(f"âš ï¸ åˆ é™¤NCå¤±è´¥({reason}): {exc}")

    def drain_completed(block: bool) -> None:
        nonlocal processed
        if not parallel or not active_futures:
            return

        futures = list(active_futures.keys())
        if block:
            done, _ = wait(futures, return_when=FIRST_COMPLETED)
        else:
            done = {f for f in futures if f.done()}
            if not done:
                return

        for fut in done:
            meta = active_futures.pop(fut, {})
            label = meta.get("label", "æœªçŸ¥æ–‡ä»¶")
            try:
                success, error_msg, produced = fut.result()
            except Exception as exc:  # pragma: no cover - defensive
                success = False
                error_msg = str(exc)
                produced = set()
            if success:
                processed += 1
                summary(f"âœ… ç¯å¢ƒåˆ†æå®Œæˆ: {label}")
                stem = meta.get("stem")
                if stem:
                    _register_manifest_entries(
                        existing_index, final_output_dir, stem, produced
                    )
            else:
                log_hint = meta.get("log")
                extra = f" -> {error_msg}" if error_msg else ""
                if log_hint:
                    summary(
                        f"âŒ ç¯å¢ƒåˆ†æå¤±è´¥: {label}{extra} (è¯¦è§ {log_hint})"
                    )
                else:
                    summary(f"âŒ ç¯å¢ƒåˆ†æå¤±è´¥: {label}{extra}")

    def ensure_capacity() -> None:
        if not parallel:
            return
        while len(active_futures) >= max_in_flight:
            drain_completed(block=True)

    processed = 0
    skipped = pre_skipped
    for idx, nc_file in enumerate(target_nc_files, start=1):
        import re

        if parallel:
            drain_completed(block=False)
            ensure_capacity()

        nc_stem = nc_file.stem
        detail(f"\n[{idx}/{len(target_nc_files)}] â–¶ï¸ å¤„ç† NC: {nc_file.name}")
        existing_particles = existing_index.get(nc_stem, set())
        if existing_particles:
            detail(f"â­ï¸  å·²å­˜åœ¨åˆ†æç»“æœ ({len(existing_particles)}) -> è·³è¿‡ {nc_stem}")
            skipped += 1
            continue

        track_file = None
        if args.tracks:
            t = Path(args.tracks)
            if t.exists():
                track_file = t
        if track_file is None:
            tdir = Path("track_single")
            if tdir.exists():
                normalized_tag = extract_forecast_tag(nc_stem)
                exact_candidates: list[Path] = []
                exact_candidates.extend(sorted(tdir.glob(f"track_*_{nc_stem}.csv")))
                exact_candidates.extend(sorted(tdir.glob(f"tracks_*_{nc_stem}_*.csv")))
                tag_candidates = []
                if normalized_tag != "track":
                    tag_candidates = sorted(
                        tdir.glob(f"tracks_*_{normalized_tag}.csv")
                    )
                if exact_candidates:
                    track_file = exact_candidates[0]
                elif tag_candidates:
                    track_file = tag_candidates[0]
                else:
                    leftover = sorted(tdir.glob("tracks_*.csv"))
                    if leftover:
                        summary(
                            "âš ï¸ æ‰¾åˆ°ç°æœ‰è½¨è¿¹æ–‡ä»¶ä½†ä¸å½“å‰ NC ä¸åŒ¹é…ï¼Œå·²å¿½ç•¥å¹¶é‡æ–°è¿½è¸ª"
                        )
        if track_file is None:
            if args.auto:
                from initialTracker import track_file_with_initials as it_track_file_with_initials
                from initialTracker import _load_all_points as it_load_initial_points

                detail("ğŸ”„ ä½¿ç”¨ initialTracker è‡ªåŠ¨è¿½è¸ªå½“å‰NCä»¥ç”Ÿæˆè½¨è¿¹...")
                try:
                    initials_path = (
                        Path(args.initials)
                        if args.initials
                        else Path("input/western_pacific_typhoons_superfast.csv")
                    )
                    initials_df = it_load_initial_points(initials_path)
                    out_dir = Path("track_single")
                    out_dir.mkdir(exist_ok=True)
                    per_storm = it_track_file_with_initials(Path(nc_file), initials_df, out_dir)
                    if not per_storm:
                        detail("âš ï¸ æ— è½¨è¿¹ -> è·³è¿‡è¯¥NC")
                        remove_nc_file(nc_file, "æ— è½¨è¿¹")
                        skipped += 1
                        continue
                    combined = combine_initial_tracker_outputs(per_storm, nc_file)
                    if combined is None or combined.empty:
                        detail("âš ï¸ è‡ªåŠ¨è¿½è¸ªæ— æœ‰æ•ˆè½¨è¿¹ -> è·³è¿‡è¯¥NC")
                        remove_nc_file(nc_file, "æ— è½¨è¿¹")
                        skipped += 1
                        continue
                    first_time = (
                        combined.iloc[0]["time"] if "time" in combined.columns else None
                    )
                    ts0 = (
                        pd.to_datetime(first_time).strftime("%Y%m%d%H")
                        if pd.notnull(first_time)
                        else "T000"
                    )
                    if combined["particle"].nunique() == 1:
                        track_file = Path(per_storm[0])
                        combined.to_csv(track_file, index=False)
                        detail(f"ğŸ’¾ è‡ªåŠ¨è½¨è¿¹æ–‡ä»¶: {track_file.name} (å•æ¡è·¯å¾„)")
                    else:
                        track_file = out_dir / f"tracks_auto_{nc_stem}_{ts0}.csv"
                        combined.to_csv(track_file, index=False)
                        detail(
                            f"ğŸ’¾ è‡ªåŠ¨è½¨è¿¹æ–‡ä»¶: {track_file.name} (å« {combined['particle'].nunique()} æ¡è·¯å¾„)"
                        )
                except Exception as e:
                    summary(f"âŒ è‡ªåŠ¨è¿½è¸ªå¤±è´¥: {e}")
                    remove_nc_file(nc_file, "è¿½è¸ªå¤±è´¥")
                    skipped += 1
                    continue
            else:
                detail("âš ï¸ æœªæ‰¾åˆ°å¯¹åº”è½¨è¿¹ä¸”æœªå¯ç”¨ --auto, è·³è¿‡")
                remove_nc_file(nc_file, "æ— è½¨è¿¹")
                skipped += 1
                continue

        detail(f"âœ… ä½¿ç”¨è½¨è¿¹æ–‡ä»¶: {track_file}")
        if parallel and executor:
            detail("ğŸ§® å·²æäº¤ç¯å¢ƒåˆ†æä»»åŠ¡ (å¹¶è¡Œ)")
            log_file = (
                str((logs_dir / f"{nc_file.stem}.log").resolve())
                if logs_dir is not None and not concise_log
                else None
            )
            future = executor.submit(
                _run_environment_analysis,
                str(nc_file),
                str(track_file),
                "final_single_output",
                keep_nc_flag,
                log_file,
                concise_log,
            )
            meta: dict[str, str] = {"label": nc_file.name, "stem": nc_stem}
            if log_file:
                meta["log"] = log_file
            active_futures[future] = meta
        else:
            try:
                success, error_msg, produced = _run_environment_analysis(
                    str(nc_file),
                    str(track_file),
                    "final_single_output",
                    keep_nc_flag,
                    None,
                    concise_log,
                )
                if success:
                    processed += 1
                    _register_manifest_entries(
                        existing_index, final_output_dir, nc_stem, produced
                    )
                elif error_msg:
                    summary(f"âŒ åˆ†æå¤±è´¥ {nc_file.name}: {error_msg}")
            except Exception as e:
                summary(f"âŒ åˆ†æå¤±è´¥ {nc_file.name}: {e}")
                continue

    if parallel and executor:
        while active_futures:
            drain_completed(block=True)
        executor.shutdown(wait=True)

    summary("\nğŸ‰ å¤šæ–‡ä»¶ç¯å¢ƒåˆ†æå®Œæˆ. ç»Ÿè®¡:")
    summary(f"  âœ… å·²åˆ†æ: {processed}")
    summary(f"  â­ï¸ è·³è¿‡(å·²æœ‰ç»“æœ/æ— è½¨è¿¹): {skipped}")
    summary(f"  ğŸ“¦ å®é™…éå†: {len(target_nc_files)} / åŸå§‹ {original_total}")
    summary("ç»“æœç›®å½•: final_single_output")

    return processed, skipped
