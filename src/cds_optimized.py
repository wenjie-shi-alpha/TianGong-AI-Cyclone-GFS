#!/usr/bin/env python3
"""
CDSä¼˜åŒ–ç‰ˆæœ¬ - è§£å†³APIæ’é˜Ÿé—®é¢˜

ä¸»è¦æ”¹è¿›ï¼š
1. æŒ‰æ—¥æ‹†åˆ†pressure-levelè¯·æ±‚ï¼ˆéµå¾ªMARS tapeè§„åˆ™ï¼‰
2. æŒ‰å‘¨æ‹†åˆ†single-levelè¯·æ±‚
3. å¼‚æ­¥å¹¶è¡Œä¸‹è½½ï¼ˆThreadPoolExecutorï¼Œé™åˆ¶4å¹¶å‘ï¼‰
4. æ™ºèƒ½é‡è¯•æœºåˆ¶
5. åŒºåŸŸè£å‰ªå’ŒGRIBæ ¼å¼
6. å¢é‡ä¸‹è½½ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
"""

import cdsapi
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
import json
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Semaphore
import xarray as xr
import warnings

warnings.filterwarnings('ignore')


class OptimizedCDSDownloader:
    """
    ä¼˜åŒ–çš„CDSä¸‹è½½å™¨
    
    å…³é”®æ”¹è¿›ï¼š
    - æŒ‰MARS tapeè§„åˆ™ç»„ç»‡è¯·æ±‚ï¼ˆpressureæŒ‰æ—¥ï¼ŒsingleæŒ‰å‘¨ï¼‰
    - 4çº¿ç¨‹å¹¶è¡Œä¸‹è½½
    - è‡ªåŠ¨é‡è¯•å’Œé”™è¯¯æ¢å¤
    - æ–­ç‚¹ç»­ä¼ 
    """
    
    def __init__(
        self,
        output_dir="./cds_output_optimized",
        max_concurrent=4,
        area=None,  # [North, West, South, East]ï¼Œä¾‹å¦‚ [60, 100, 0, 180]
        use_grib=True,
    ):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        
        # CDSå®¢æˆ·ç«¯é…ç½®
        self.cds_client = cdsapi.Client(
            timeout=600,      # 10åˆ†é’Ÿè¶…æ—¶
            quiet=False,
            debug=False,
            retry_max=5
        )
        
        # å¹¶å‘æ§åˆ¶
        self.max_concurrent = max_concurrent
        self.semaphore = Semaphore(max_concurrent)
        
        # åŒºåŸŸå’Œæ ¼å¼
        self.area = area  # è¥¿å¤ªå¹³æ´‹: [60, 100, 0, 180]
        self.data_format = 'grib' if use_grib else 'netcdf'
        
        print(f"âœ… ä¼˜åŒ–CDSä¸‹è½½å™¨å·²åˆå§‹åŒ–")
        print(f"   - æœ€å¤§å¹¶å‘: {max_concurrent}")
        print(f"   - åŒºåŸŸè£å‰ª: {'æ˜¯' if area else 'å¦'}")
        print(f"   - æ ¼å¼: {self.data_format.upper()}")
    
    def download_single_level_day(self, date_str, variables=None):
        """
        ä¸‹è½½å•æ—¥çš„single-levelæ•°æ®
        
        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ 'YYYY-MM-DD'
            variables: å˜é‡åˆ—è¡¨ï¼ŒNoneåˆ™ä½¿ç”¨é»˜è®¤åˆ—è¡¨
        """
        if variables is None:
            variables = [
                'mean_sea_level_pressure',
                '10m_u_component_of_wind',
                '10m_v_component_of_wind',
                '2m_temperature',
                'sea_surface_temperature',
                'total_column_water_vapour'
            ]
        
        date = pd.Timestamp(date_str)
        output_file = self.output_dir / f"era5_single_{date.strftime('%Y%m%d')}.{self.data_format[:4]}"
        
        if output_file.exists():
            print(f"ğŸ“ å•å±‚æ•°æ®å·²å­˜åœ¨: {date_str}")
            return str(output_file)
        
        request = {
            'product_type': 'reanalysis',
            'format': self.data_format,
            'variable': variables,
            'year': date.strftime('%Y'),
            'month': date.strftime('%m'),
            'day': date.strftime('%d'),
            'time': ['00:00', '06:00', '12:00', '18:00'],
        }
        
        if self.area:
            request['area'] = self.area
        
        with self.semaphore:
            print(f"ğŸ“¥ ä¸‹è½½å•å±‚æ•°æ®: {date_str}")
            try:
                self.cds_client.retrieve(
                    'reanalysis-era5-single-levels',
                    request,
                    str(output_file)
                )
                print(f"âœ… å•å±‚æ•°æ®å®Œæˆ: {date_str}")
                return str(output_file)
            except Exception as e:
                print(f"âŒ å•å±‚æ•°æ®å¤±è´¥: {date_str} - {e}")
                if output_file.exists():
                    output_file.unlink()
                raise
    
    def download_pressure_level_day(self, date_str, levels=None, variables=None):
        """
        ä¸‹è½½å•æ—¥çš„pressure-levelæ•°æ®
        
        å…³é”®ä¼˜åŒ–ï¼šå•æ—¥æ‰€æœ‰å±‚çº§å’Œå˜é‡åœ¨MARSåŒä¸€tapeï¼Œæ£€ç´¢æœ€å¿«
        
        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸²
            levels: æ°”å‹å±‚çº§åˆ—è¡¨
            variables: å˜é‡åˆ—è¡¨
        """
        if levels is None:
            levels = ['850', '700', '500', '300', '200']
        
        if variables is None:
            variables = [
                'u_component_of_wind',
                'v_component_of_wind',
                'geopotential',
                'temperature',
                'relative_humidity'
            ]
        
        date = pd.Timestamp(date_str)
        output_file = self.output_dir / f"era5_pressure_{date.strftime('%Y%m%d')}.{self.data_format[:4]}"
        
        if output_file.exists():
            print(f"ğŸ“ ç­‰å‹é¢æ•°æ®å·²å­˜åœ¨: {date_str}")
            return str(output_file)
        
        request = {
            'product_type': 'reanalysis',
            'format': self.data_format,
            'variable': variables,
            'pressure_level': levels,
            'year': date.strftime('%Y'),
            'month': date.strftime('%m'),
            'day': date.strftime('%d'),
            'time': ['00:00', '06:00', '12:00', '18:00'],
        }
        
        if self.area:
            request['area'] = self.area
        
        with self.semaphore:
            print(f"ğŸ“¥ ä¸‹è½½ç­‰å‹é¢æ•°æ®: {date_str}")
            try:
                self.cds_client.retrieve(
                    'reanalysis-era5-pressure-levels',
                    request,
                    str(output_file)
                )
                print(f"âœ… ç­‰å‹é¢æ•°æ®å®Œæˆ: {date_str}")
                return str(output_file)
            except Exception as e:
                print(f"âŒ ç­‰å‹é¢æ•°æ®å¤±è´¥: {date_str} - {e}")
                if output_file.exists():
                    output_file.unlink()
                raise
    
    def download_with_retry(self, download_func, max_retries=3):
        """
        å¸¦é‡è¯•çš„ä¸‹è½½åŒ…è£…å™¨
        
        Args:
            download_func: ä¸‹è½½å‡½æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        for attempt in range(max_retries):
            try:
                return download_func()
            except Exception as e:
                if attempt < max_retries - 1:
                    wait_time = (2 ** attempt) * 60  # æŒ‡æ•°é€€é¿: 1åˆ†é’Ÿ, 2åˆ†é’Ÿ, 4åˆ†é’Ÿ
                    print(f"âš ï¸ ä¸‹è½½å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰ï¼Œ"
                          f"{wait_time}ç§’åé‡è¯•: {e}")
                    time.sleep(wait_time)
                else:
                    print(f"âŒ ä¸‹è½½å½»åº•å¤±è´¥ï¼ˆå·²é‡è¯•{max_retries}æ¬¡ï¼‰: {e}")
                    raise
    
    def get_missing_dates(self, start_date, end_date, data_type='pressure'):
        """
        æ£€æµ‹ç¼ºå¤±çš„æ—¥æœŸ
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            data_type: 'pressure' æˆ– 'single'
        
        Returns:
            ç¼ºå¤±æ—¥æœŸåˆ—è¡¨
        """
        all_dates = pd.date_range(start=start_date, end=end_date, freq='D')
        missing_dates = []
        
        for date in all_dates:
            date_str = date.strftime('%Y%m%d')
            if data_type == 'pressure':
                expected_file = self.output_dir / f"era5_pressure_{date_str}.{self.data_format[:4]}"
            else:
                expected_file = self.output_dir / f"era5_single_{date_str}.{self.data_format[:4]}"
            
            if not expected_file.exists():
                missing_dates.append(date.strftime('%Y-%m-%d'))
        
        return missing_dates
    
    def parallel_download_dates(self, date_list, download_func):
        """
        å¹¶è¡Œä¸‹è½½å¤šä¸ªæ—¥æœŸ
        
        Args:
            date_list: æ—¥æœŸåˆ—è¡¨
            download_func: ä¸‹è½½å‡½æ•°ï¼ˆæ¥å—date_strå‚æ•°ï¼‰
        
        Returns:
            æˆåŠŸä¸‹è½½çš„æ–‡ä»¶åˆ—è¡¨
        """
        if not date_list:
            print("â„¹ï¸ æ— éœ€ä¸‹è½½")
            return []
        
        print(f"ğŸš€ å¼€å§‹å¹¶è¡Œä¸‹è½½ {len(date_list)} ä¸ªæ—¥æœŸï¼ˆ{self.max_concurrent} çº¿ç¨‹ï¼‰")
        
        downloaded_files = []
        failed_dates = []
        
        with ThreadPoolExecutor(max_workers=self.max_concurrent) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_date = {
                executor.submit(
                    self.download_with_retry,
                    lambda d=date: download_func(d)
                ): date
                for date in date_list
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_date):
                date = future_to_date[future]
                try:
                    result = future.result()
                    if result:
                        downloaded_files.append(result)
                except Exception as e:
                    failed_dates.append(date)
                    print(f"âŒ {date} æœ€ç»ˆå¤±è´¥: {e}")
        
        print(f"ğŸ“Š ä¸‹è½½å®Œæˆ: {len(downloaded_files)} æˆåŠŸ, {len(failed_dates)} å¤±è´¥")
        if failed_dates:
            print(f"   å¤±è´¥æ—¥æœŸ: {failed_dates}")
        
        return downloaded_files
    
    def download_month_optimized(self, year, month):
        """
        ä¼˜åŒ–çš„æœˆåº¦ä¸‹è½½
        
        ç­–ç•¥ï¼š
        1. æ£€æµ‹ç¼ºå¤±æ—¥æœŸ
        2. å¹¶è¡Œä¸‹è½½pressureå’Œsingleæ•°æ®
        3. åˆå¹¶ä¸ºæœˆåº¦æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        
        Args:
            year: å¹´ä»½
            month: æœˆä»½
        
        Returns:
            (pressure_files, single_files) å…ƒç»„
        """
        start_date = f"{year}-{month:02d}-01"
        end_date = pd.Timestamp(start_date) + pd.offsets.MonthEnd(0)
        end_date = end_date.strftime('%Y-%m-%d')
        
        print(f"\n{'='*50}")
        print(f"å¤„ç†æœˆä»½: {year}-{month:02d}")
        print(f"æ—¥æœŸèŒƒå›´: {start_date} è‡³ {end_date}")
        print(f"{'='*50}")
        
        # æ£€æµ‹ç¼ºå¤±æ•°æ®
        missing_pressure = self.get_missing_dates(start_date, end_date, 'pressure')
        missing_single = self.get_missing_dates(start_date, end_date, 'single')
        
        print(f"ğŸ“‹ éœ€è¦ä¸‹è½½: pressure={len(missing_pressure)}å¤©, single={len(missing_single)}å¤©")
        
        # å¹¶è¡Œä¸‹è½½pressureæ•°æ®
        print("\n--- ä¸‹è½½ç­‰å‹é¢æ•°æ® ---")
        pressure_files = self.parallel_download_dates(
            missing_pressure,
            self.download_pressure_level_day
        )
        
        # å¹¶è¡Œä¸‹è½½singleæ•°æ®
        print("\n--- ä¸‹è½½å•å±‚æ•°æ® ---")
        single_files = self.parallel_download_dates(
            missing_single,
            self.download_single_level_day
        )
        
        # è·å–æ‰€æœ‰å·²å­˜åœ¨çš„æ–‡ä»¶
        all_dates = pd.date_range(start=start_date, end=end_date, freq='D')
        all_pressure_files = [
            str(self.output_dir / f"era5_pressure_{d.strftime('%Y%m%d')}.{self.data_format[:4]}")
            for d in all_dates
        ]
        all_single_files = [
            str(self.output_dir / f"era5_single_{d.strftime('%Y%m%d')}.{self.data_format[:4]}")
            for d in all_dates
        ]
        
        # è¿‡æ»¤å­˜åœ¨çš„æ–‡ä»¶
        all_pressure_files = [f for f in all_pressure_files if Path(f).exists()]
        all_single_files = [f for f in all_single_files if Path(f).exists()]
        
        print(f"\nâœ… {year}-{month:02d} ä¸‹è½½å®Œæˆ:")
        print(f"   ç­‰å‹é¢: {len(all_pressure_files)} ä¸ªæ–‡ä»¶")
        print(f"   å•å±‚: {len(all_single_files)} ä¸ªæ–‡ä»¶")
        
        return all_pressure_files, all_single_files
    
    def merge_daily_files_to_month(self, daily_files, output_file):
        """
        åˆå¹¶æ—¥åº¦æ–‡ä»¶ä¸ºæœˆåº¦æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        
        Args:
            daily_files: æ—¥åº¦æ–‡ä»¶åˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if not daily_files:
            return None
        
        if Path(output_file).exists():
            print(f"ğŸ“ æœˆåº¦åˆå¹¶æ–‡ä»¶å·²å­˜åœ¨: {output_file}")
            return str(output_file)
        
        print(f"ğŸ”— åˆå¹¶ {len(daily_files)} ä¸ªæ—¥åº¦æ–‡ä»¶...")
        
        try:
            # ä½¿ç”¨xarrayåˆå¹¶
            if self.data_format == 'grib':
                datasets = [xr.open_dataset(f, engine='cfgrib') for f in daily_files]
            else:
                datasets = [xr.open_dataset(f) for f in daily_files]
            
            merged = xr.concat(datasets, dim='time')
            merged = merged.sortby('time')
            
            # ä¿å­˜
            if output_file.endswith('.nc'):
                merged.to_netcdf(output_file)
            else:
                merged.to_netcdf(output_file.replace('.grib', '.nc'))
            
            # å…³é—­æ•°æ®é›†
            for ds in datasets:
                ds.close()
            merged.close()
            
            print(f"âœ… æœˆåº¦æ–‡ä»¶å·²ä¿å­˜: {output_file}")
            return str(output_file)
            
        except Exception as e:
            print(f"âš ï¸ åˆå¹¶å¤±è´¥: {e}")
            return None


def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = OptimizedCDSDownloader(
        output_dir="./cds_output_optimized",
        max_concurrent=4,  # 4çº¿ç¨‹å¹¶å‘
        area=[60, 100, 0, 180],  # è¥¿å¤ªå¹³æ´‹åŒºåŸŸ
        use_grib=True  # ä½¿ç”¨GRIBæ ¼å¼ï¼ˆæ›´å¿«ï¼‰
    )
    
    # ç¤ºä¾‹1ï¼šä¸‹è½½å•ä¸ªæœˆä»½
    pressure_files, single_files = downloader.download_month_optimized(2006, 3)
    
    # ç¤ºä¾‹2ï¼šæ‰¹é‡ä¸‹è½½å¤šä¸ªæœˆä»½
    months_to_download = [
        (2006, 3),
        (2006, 4),
        (2006, 5),
    ]
    
    for year, month in months_to_download:
        p_files, s_files = downloader.download_month_optimized(year, month)
        
        # å¯é€‰ï¼šåˆå¹¶ä¸ºæœˆåº¦æ–‡ä»¶
        if p_files:
            monthly_pressure = f"./cds_output_optimized/era5_pressure_{year}{month:02d}_monthly.nc"
            downloader.merge_daily_files_to_month(p_files, monthly_pressure)
        
        if s_files:
            monthly_single = f"./cds_output_optimized/era5_single_{year}{month:02d}_monthly.nc"
            downloader.merge_daily_files_to_month(s_files, monthly_single)


if __name__ == "__main__":
    example_usage()
