# CDS 数据下载优化 - 按年份分批

## 📋 修改说明

已将 `cds.py` 脚本修改为**按年份分批下载**模式，解决了单次请求数据量过大导致的 `cost limits exceeded` 错误。

## 🔄 新的工作流程

### 第一阶段：按年份下载数据
```
对于 2021-2025 年的数据：
├─ 2021年: 下载单层数据 + 等压面数据 (2个API请求)
├─ 2022年: 下载单层数据 + 等压面数据 (2个API请求)
├─ 2023年: 下载单层数据 + 等压面数据 (2个API请求)
├─ 2024年: 下载单层数据 + 等压面数据 (2个API请求)
└─ 2025年: 下载单层数据 + 等压面数据 (2个API请求)

总计: 10个API请求（5年 × 2类数据）
```

### 第二阶段：处理数据
```
1. 合并所有年份的数据文件
2. 按月处理路径点
3. 每个月保存一个JSON结果文件
4. 处理完成后清理下载的数据文件
```

## ✨ 主要特性

### 1. 按年份切分下载
- **原来**: 一次请求5年数据 → 被拒绝（数据量过大）
- **现在**: 每年单独请求 → 每个请求更小，更容易成功

### 2. 自动合并数据
- 使用 `xarray.concat` 自动合并多个年份的数据文件
- 对用户透明，处理逻辑保持不变

### 3. 智能文件管理
- 每年的数据分别存储为独立的 NetCDF 文件
- 文件命名规则: `era5_single_YYYYMMDD_YYYYMMDD.nc`
- 处理完成后可选择自动清理

### 4. 断点续传
- 如果某个年份的数据已存在，会跳过下载
- 可以中断后继续，无需重新下载已有数据

## 📊 API 请求优化对比

| 时间范围 | 原方案 | 现方案 | 说明 |
|---------|--------|--------|------|
| 1年数据 | 2个请求 | 2个请求 | 相同 |
| 5年数据 | 2个请求 | 10个请求 | 但每个请求更小，更可靠 |
| 10年数据 | 2个请求 | 20个请求 | 避免超限错误 |

**关键优势**: 虽然请求数量增加，但每个请求的数据量减小，避免了 CDS 的 cost limits 限制。

## 🚀 使用示例

```bash
# 基本用法（自动按年份下载）
python cds.py --tracks input/matched_cyclone_tracks.csv --output ./cds_output

# 指定工作线程数
python cds.py --tracks input/matched_cyclone_tracks.csv --workers 4

# 保留中间文件（不自动清理）
python cds.py --tracks input/matched_cyclone_tracks.csv --no-clean
```

## 📁 输出文件结构

```
cds_output/
├── era5_single_20210128_20211231.nc      # 2021年单层数据
├── era5_pressure_20210128_20211231.nc    # 2021年等压面数据
├── era5_single_20220101_20221231.nc      # 2022年单层数据
├── era5_pressure_20220101_20221231.nc    # 2022年等压面数据
├── ...
├── cds_environment_analysis_2021-01.json # 2021年1月结果
├── cds_environment_analysis_2021-02.json # 2021年2月结果
└── ...
```

## 💡 运行输出示例

```
============================================================
第一阶段：按年份下载所有ERA5数据
============================================================
🗓️ 数据时间范围: 2021-01-28 到 2025-07-31
📊 共 15000 个路径点
📅 将按年份下载，共 5 年: [2021, 2022, 2023, 2024, 2025]

========================= 下载 2021 年数据 =========================
   时间范围: 2021-01-28 到 2021-12-31
   路径点数: 3200
📥 下载ERA5单层数据: 2021-01-28 到 2021-12-31
   年份: ['2021']
   月份: ['01', '02', ..., '12']
   请求天数: 31 天
✅ ERA5单层数据下载完成: ./cds_output/era5_single_20210128_20211231.nc

📥 下载ERA5等压面数据: 2021-01-28 到 2021-12-31
   年份: ['2021']
   月份: ['01', '02', ..., '12']
   请求天数: 31 天
✅ ERA5等压面数据下载完成: ./cds_output/era5_pressure_20210128_20211231.nc

========================= 下载 2022 年数据 =========================
...

✅ 所有数据下载完成！
   单层数据文件: 5 个
   等压面数据文件: 5 个

============================================================
第二阶段：处理数据并按月保存结果
============================================================
合并并加载数据...
📥 加载 5 个单层数据文件...
📥 加载 5 个等压面数据文件...
📊 ERA5数据加载完成: {'time': 43800, 'latitude': 721, 'longitude': 1440}

开始处理数据（按月保存结果）
🗓️ 将处理 55 个月份: ['2021-01', '2021-02', ...]
...
```

## ⚙️ 技术细节

### 数据合并方式
```python
# 合并单层数据
ds_single_list = [xr.open_dataset(f) for f in single_files]
ds_single_merged = xr.concat(ds_single_list, dim='time')

# 合并等压面数据
ds_pressure_list = [xr.open_dataset(f) for f in pressure_files]
ds_pressure_merged = xr.concat(ds_pressure_list, dim='time')

# 合并单层和等压面
self.ds = xr.merge([ds_single_merged, ds_pressure_merged])
```

### 年份识别
```python
years = sorted(list(set(self.tracks_df['time'].dt.year)))
for year in years:
    year_data = self.tracks_df[self.tracks_df['time'].dt.year == year]
    year_start = year_data['time'].min().strftime('%Y-%m-%d')
    year_end = year_data['time'].max().strftime('%Y-%m-%d')
```

## 🔧 故障排除

### 问题1: 某个年份下载失败
**解决**: 重新运行脚本，已下载的年份会自动跳过

### 问题2: 内存不足
**解决**: 
- 减少并行工作线程: `--workers 1`
- 设置 xarray chunks: `export CDS_XR_CHUNKS="time:1,latitude:200,longitude:200"`

### 问题3: API配额限制
**解决**: 
- 分批运行，每次处理少量年份
- 等待一段时间后继续

## 📝 更新日志

- **2024-10-31**: 修改为按年份下载，解决 cost limits exceeded 错误
- 支持多个年份文件的自动合并
- 优化输出日志，更清晰地显示进度

