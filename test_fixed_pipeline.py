#!/usr/bin/env python
"""
éªŒè¯ä¿®å¤åçš„å®Œæ•´ç®¡é“ï¼š
1. åŠ è½½å¤šé«˜åº¦é£åœºå’Œæ¸©åº¦æ•°æ®
2. è¿½è¸ªæ°”æ—‹
3. æå–ç¯å¢ƒç³»ç»Ÿ
"""

import sys
from pathlib import Path

import numpy as np
import pandas as pd
import xarray as xr

PROJECT_PATH = Path(__file__).parent
sys.path.insert(0, str(PROJECT_PATH / "src"))

from initial_tracker.dataset_adapter import _DsAdapter, _build_batch_from_ds_fast
from initial_tracker.initials import _load_all_points, _select_initials_for_time
from initial_tracker.tracker import Tracker
from initial_tracker.exceptions import NoEyeException
from environment_extractor.extractor import TCEnvironmentalSystemsExtractor

# ============================================================================
# é…ç½®
# ============================================================================
DATASET_URL = "gs://weatherbench2/datasets/hres_t0/2016-2022-6h-1440x721.zarr"
TIME_RANGE = ("2020-07-25", "2020-08-05")
LAT_RANGE = (-5.0, 45.0)
LON_RANGE = (100.0, 180.0)  # æ‰©å±•ä»¥åŒ…å«æ‰€æœ‰æ°”æ—‹å€™é€‰

OUTPUT_DIR = PROJECT_PATH / "colab_outputs_local"
INITIALS_CSV = PROJECT_PATH / "input" / "western_pacific_typhoons_superfast.csv"

print("\n" + "="*70)
print("ã€å®Œæ•´æµ‹è¯•ã€‘ç¯å¢ƒç³»ç»Ÿæå–ä¿®å¤éªŒè¯")
print("="*70)

# ============================================================================
# 1. åŠ è½½æ•°æ® - åŒ…å«å¤šé«˜åº¦é£åœº
# ============================================================================
print("\nğŸ“¥ 1. åŠ è½½ WeatherBench 2 æ•°æ®ï¼ˆå«å¤šé«˜åº¦é£åœºï¼‰...")
rename_map = {
    "mean_sea_level_pressure": "msl",
    "10m_u_component_of_wind": "u10",
    "10m_v_component_of_wind": "v10",
    "u_component_of_wind": "u",        # âœ… å¤šé«˜åº¦é£ (å¿…éœ€)
    "v_component_of_wind": "v",        # âœ… å¤šé«˜åº¦é£ (å¿…éœ€)
    "temperature": "t",                # å¤šé«˜åº¦æ¸©åº¦
    "specific_humidity": "q",
    "geopotential": "z",
    "land_sea_mask": "lsm",
    "2m_temperature": "t2m",
}

ds_raw = xr.open_zarr(
    DATASET_URL,
    consolidated=True,
    storage_options={"token": "anon"},
)

present = {src: dst for src, dst in rename_map.items() if src in ds_raw}
ds = ds_raw[list(present.keys())].rename(present)

# å•ä½è½¬æ¢
ds["z"] = ds["z"] / 9.80665  # m^2/s^2 -> geopotential height (m)

# åˆæˆ LSM
if "lsm" not in ds:
    n_lat, n_lon = len(ds.latitude), len(ds.longitude)
    ds["lsm"] = xr.DataArray(
        np.zeros((n_lat, n_lon), dtype=np.float32),
        coords={"latitude": ds.latitude, "longitude": ds.longitude},
        dims=["latitude", "longitude"],
    )

# ç©ºé—´-æ—¶é—´åˆ‡ç‰‡
ds = ds.sel(
    latitude=slice(-5, 45),
    longitude=slice(100, 180),
    time=slice(*TIME_RANGE),
).chunk({"time": 1, "latitude": 181, "longitude": 361})

print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
print(f"   Shape: time={len(ds.time)}, lat={len(ds.latitude)}, lon={len(ds.longitude)}")
print(f"   Memory: {ds.nbytes/1e9:.2f} GB")
print(f"   Variables: {list(ds.data_vars)}")
if "u" in ds.data_vars:
    print(f"   âœ… å¤šé«˜åº¦é£æ•°æ®å·²åŠ è½½ (levels: {sorted(ds.level.values)})")

# ============================================================================
# 2. æ°”æ—‹è¿½è¸ª
# ============================================================================
print("\nğŸŒªï¸ 2. æ‰§è¡Œæ°”æ—‹è¿½è¸ª...")

def _normalize_lon_for_grid(lon_value: float, lon_grid: np.ndarray) -> float:
    lon = float(lon_value)
    grid_min = float(lon_grid.min())
    grid_max = float(lon_grid.max())
    if grid_min >= 0 and lon < 0:
        lon = lon % 360
    if grid_max <= 180 and lon > 180:
        lon = ((lon + 180) % 360) - 180
    return lon

adapter = _DsAdapter.build(ds)
times = pd.Index(adapter.times)
start_time = pd.Timestamp("2020-08-01 00:00")
start_idx = int(np.argmin(np.abs(times - start_time)))

all_initials = _load_all_points(INITIALS_CSV)
init_candidates = _select_initials_for_time(all_initials, times[start_idx], tol_hours=6)

print(f"âœ… å‘ç° {len(init_candidates)} ä¸ªæ°”æ—‹å€™é€‰")

tracks = {}
for _, row in init_candidates.sort_values("storm_id").head(2).iterrows():  # åªè¿½è¸ªå‰2ä¸ª
    storm_id = str(row["storm_id"])
    init_lat = float(row["init_lat"])
    init_lon = _normalize_lon_for_grid(float(row["init_lon"]), adapter.lons)
    
    tracker = Tracker(
        init_lat=init_lat,
        init_lon=init_lon,
        init_time=times[start_idx],
        init_msl=None,
        init_wind=None,
    )
    
    for time_idx in range(start_idx, min(start_idx + 30, len(adapter.times))):
        batch = _build_batch_from_ds_fast(adapter, time_idx)
        try:
            tracker.step(batch)
        except NoEyeException:
            if time_idx == start_idx:
                tracker = None
                break
            continue
        if tracker.dissipated:
            break
    
    if tracker is not None:
        df = tracker.results()
        df["storm_id"] = storm_id
        tracks[storm_id] = df
        print(f"   âœ… {storm_id}: {len(df)} ä¸ªè¿½è¸ªç‚¹")

# ============================================================================
# 3. ç¯å¢ƒæå–
# ============================================================================
print("\nğŸŒŠ 3. ç¯å¢ƒç³»ç»Ÿæå–...")

OUTPUT_DIR.mkdir(exist_ok=True)

for storm_id, track_df in tracks.items():
    print(f"\n   å¤„ç† {storm_id}...")
    
    # æå–å­é›†
    lat_vals = track_df["lat"].astype(float)
    lon_vals = track_df["lon"].astype(float)
    lat_min = max(lat_vals.min() - 8, float(ds.latitude.values.min()))
    lat_max = min(lat_vals.max() + 8, float(ds.latitude.values.max()))
    lon_min = max(lon_vals.min() - 8, float(ds.longitude.values.min()))
    lon_max = min(lon_vals.max() + 8, float(ds.longitude.values.max()))
    
    times_track = pd.to_datetime(track_df["time"])
    time_slice = slice(times_track.min() - pd.Timedelta(hours=12), times_track.max() + pd.Timedelta(hours=12))
    
    ds_subset = ds.sel(
        latitude=slice(lat_min, lat_max),
        longitude=slice(lon_min, lon_max),
        time=time_slice,
    )
    
    # ä¿å­˜åˆ° NetCDF
    nc_path = OUTPUT_DIR / "nc_subsets" / f"{storm_id}_subset_fixed.nc"
    nc_path.parent.mkdir(parents=True, exist_ok=True)
    
    encoding = {
        name: {"dtype": "float32", "zlib": True, "complevel": 4}
        for name in ds_subset.data_vars
        if np.issubdtype(ds_subset[name].dtype, np.floating)
    }
    ds_subset.to_netcdf(nc_path, engine="netcdf4", encoding=encoding, compute=True)
    
    print(f"     âœ… NC å­é›†å·²ä¿å­˜: {nc_path.name} ({nc_path.stat().st_size/1e6:.1f} MB)")
    print(f"        å˜é‡: {list(ds_subset.data_vars)}")
    if "u" in ds_subset.data_vars:
        print(f"        âœ… å¤šé«˜åº¦é£åŒ…å«åœ¨å†… (levels: {len(ds_subset.level)})")
    
    # ä¿å­˜è¿½è¸ª CSV
    track_path = OUTPUT_DIR / "tracks_for_extractor" / f"{storm_id}_track_fixed.csv"
    track_path.parent.mkdir(parents=True, exist_ok=True)
    track_df.to_csv(track_path, index=False)
    
    # è¿è¡Œç¯å¢ƒæå–
    print(f"     ğŸ”§ å¯åŠ¨ç¯å¢ƒæå–å™¨...")
    try:
        with TCEnvironmentalSystemsExtractor(
            str(nc_path),
            str(track_path),
            enable_detailed_shape_analysis=False,
        ) as extractor:
            result = extractor.analyze_and_export_as_json(
                output_dir=str(OUTPUT_DIR / "analysis_json")
            )
        
        # æ£€æŸ¥ç»“æœ
        json_files = list((OUTPUT_DIR / "analysis_json").glob(f"{storm_id}*.json"))
        if json_files:
            import json
            with open(json_files[0]) as f:
                data = json.load(f)
            
            systems_found = 0
            for ts in data.get("time_series", []):
                systems_found += len(ts.get("environmental_systems", []))
            
            print(f"     âœ… æå–æˆåŠŸï¼å…±æ‰¾åˆ° {systems_found} ä¸ªç¯å¢ƒç³»ç»Ÿ")
        else:
            print(f"     âŒ æœªç”Ÿæˆ JSON æ–‡ä»¶")
            
    except Exception as e:
        print(f"     âŒ æå–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

print("\n" + "="*70)
print("âœ… å®Œæ•´æµ‹è¯•å®Œæˆï¼")
print("="*70)
